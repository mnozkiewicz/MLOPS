{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a852700f",
      "metadata": {
        "id": "a852700f"
      },
      "source": [
        "### Setup and model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7dd3bbb2",
      "metadata": {
        "id": "7dd3bbb2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5fc8d02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5fc8d02",
        "outputId": "c2fa9866-f948-40f8-8246-3ae161deb077"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_name = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7dde5606",
      "metadata": {
        "id": "7dde5606"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "from functools import wraps\n",
        "import numpy as np\n",
        "\n",
        "def measure_func_time(trails: int):\n",
        "\n",
        "    def decorator(func):\n",
        "\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs) -> float:\n",
        "\n",
        "            time_meaures = []\n",
        "            for i in range(trails):\n",
        "                start = perf_counter()\n",
        "                _ = func(*args, **kwargs)\n",
        "                end = perf_counter()\n",
        "                elapsed = end - start\n",
        "                time_meaures.append(elapsed)\n",
        "\n",
        "            mean_time = np.mean(time_meaures).item()\n",
        "            print(f\"[{func.__name__}] Inference time: {mean_time:.4f} seconds\")\n",
        "            return mean_time\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    return decorator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fb26064a",
      "metadata": {
        "id": "fb26064a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "@measure_func_time(trails=1000)\n",
        "@torch.no_grad()\n",
        "def inference_forward(inputs: dict[str, torch.Tensor], model: AutoModel):\n",
        "    output = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"]\n",
        "    )\n",
        "    return output\n",
        "\n",
        "\n",
        "@measure_func_time(trails=1000)\n",
        "@torch.inference_mode()\n",
        "@torch.autocast(device_type=\"cuda\")\n",
        "def autocast_forward(inputs: dict[str, torch.Tensor], model: AutoModel):\n",
        "    output = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"]\n",
        "    )\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1697ed7b",
      "metadata": {
        "id": "1697ed7b"
      },
      "source": [
        "#### Excercise 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ydmoiDZ8khMg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydmoiDZ8khMg",
        "outputId": "647936b7-ab52-4e91-f2a3-ac7eb364f6b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "text = \"Example sentence for testing onnx and onnxruntime\"\n",
        "\n",
        "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "inputs_gpu = {k: v.to(device) for k,v in inputs.items()}\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "atmAi0OZkHsJ",
      "metadata": {
        "id": "atmAi0OZkHsJ"
      },
      "outputs": [],
      "source": [
        "compiled_default = torch.compile(model)\n",
        "compiled_cudagraphs = torch.compile(model, mode=\"max-autotune\")\n",
        "compiled_no_cudagraphs = torch.compile(model, mode=\"max-autotune-no-cudagraphs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "l39oAgq-kBxR",
      "metadata": {
        "id": "l39oAgq-kBxR"
      },
      "outputs": [],
      "source": [
        "outputs = compiled_default(**inputs_gpu)\n",
        "outputs = compiled_cudagraphs(**inputs_gpu)\n",
        "outputs = compiled_no_cudagraphs(**inputs_gpu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0NsBOi8Qk-y_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NsBOi8Qk-y_",
        "outputId": "6a67fa5c-7e98-4cf7-9067-48ea7bf603ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[inference_forward] Inference time: 0.0061 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py:2450: UserWarning: Unable to hit fast path of CUDAGraphs because of pending, uninvoked backwards. Consider running with torch.no_grad() or using torch.compiler.cudagraph_mark_step_begin() before each model invocation\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[inference_forward] Inference time: 0.0092 seconds\n",
            "[inference_forward] Inference time: 0.0073 seconds\n"
          ]
        }
      ],
      "source": [
        "reference = inference_forward(inputs_gpu, compiled_default);\n",
        "cudagraphs_time = inference_forward(inputs_gpu, compiled_cudagraphs);\n",
        "no_cudagraphs_time = inference_forward(inputs_gpu, compiled_no_cudagraphs);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "MOkhLadplAJD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOkhLadplAJD",
        "outputId": "2419e4be-6b84-4cac-a608-6b96672d6504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda graphs speedup: 0.6667\n",
            "No cuda graphs speedup: 0.8438\n"
          ]
        }
      ],
      "source": [
        "print(f\"Cuda graphs speedup: {(reference / cudagraphs_time):.4f}\")\n",
        "print(f\"No cuda graphs speedup: {(reference / no_cudagraphs_time):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "FLpn4DwolmIB",
      "metadata": {
        "id": "FLpn4DwolmIB"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "In the quiet town of Eldermoor, nestled between\n",
        "rolling hills and misty forests, life had a rhythm dictated by the seasons.\n",
        "The townspeople rose with the sun, tending to gardens, markets, and workshops that had existed for generations.\n",
        "Among them, Maren, a young apothecary’s apprentice, was known not only for her curiosity but also for her relentless\n",
        "questioning of tradition. While others accepted the prescriptions handed down through centuries, Maren wondered whether\n",
        "the roots and herbs could yield more potent remedies if prepared differently, or if forgotten knowledge might be hiding\n",
        "in the old manuscripts stored in the town library’s dusty alcoves.\n",
        "\"\"\"\n",
        "\n",
        "inputs2= tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "inputs_gpu2 = {k: v.to(device) for k,v in inputs2.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "rNdmelc4l3ms",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNdmelc4l3ms",
        "outputId": "41ddfdf5-b32d-4f01-d99d-586205b9b52d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1120 00:16:53.343000 5853 torch/_inductor/utils.py:1436] [0/6] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[inference_forward] Inference time: 0.0346 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AUTOTUNE addmm(133x768, 133x768, 768x768)\n",
            "strides: [0, 1], [768, 1], [1, 768]\n",
            "dtypes: torch.float32, torch.float32, torch.float32\n",
            "  addmm 0.1351 ms 100.0% \n",
            "  bias_addmm 0.1413 ms 95.6% \n",
            "SingleProcess AUTOTUNE benchmarking takes 0.0449 seconds and 0.0003 seconds precompiling for 2 choices\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py:2450: UserWarning: Unable to hit fast path of CUDAGraphs because of pending, uninvoked backwards. Consider running with torch.no_grad() or using torch.compiler.cudagraph_mark_step_begin() before each model invocation\n",
            "  warnings.warn(\n",
            "W1120 00:18:02.236000 5853 torch/_dynamo/convert_frame.py:1016] [0/8] torch._dynamo hit config.recompile_limit (8)\n",
            "W1120 00:18:02.236000 5853 torch/_dynamo/convert_frame.py:1016] [0/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/mpnet/modeling_mpnet.py:449)\n",
            "W1120 00:18:02.236000 5853 torch/_dynamo/convert_frame.py:1016] [0/8]    last reason: 0/7: \n",
            "W1120 00:18:02.236000 5853 torch/_dynamo/convert_frame.py:1016] [0/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "W1120 00:18:02.236000 5853 torch/_dynamo/convert_frame.py:1016] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[inference_forward] Inference time: 0.0477 seconds\n",
            "[inference_forward] Inference time: 0.0130 seconds\n"
          ]
        }
      ],
      "source": [
        "reference = inference_forward(inputs_gpu2, compiled_default);\n",
        "cudagraphs_time = inference_forward(inputs_gpu2, compiled_cudagraphs);\n",
        "no_cudagraphs_time = inference_forward(inputs_gpu2, compiled_no_cudagraphs);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "rofNyqz0l60a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rofNyqz0l60a",
        "outputId": "d576bb1d-5b71-4040-837a-d17cdde75325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda graphs speedup: 0.7255\n",
            "No cuda graphs speedup: 2.6620\n"
          ]
        }
      ],
      "source": [
        "print(f\"Cuda graphs speedup: {(reference / cudagraphs_time):.4f}\")\n",
        "print(f\"No cuda graphs speedup: {(reference / no_cudagraphs_time):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FWgZfQdEmJ6y",
      "metadata": {
        "id": "FWgZfQdEmJ6y"
      },
      "source": [
        "#### Excercise 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "RdeQI8-FmG_e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdeQI8-FmG_e",
        "outputId": "9c400728-66c5-403a-b0b7-26cb826d424b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA device capability: (7, 5)\n",
            "Tensor Cores available: fast float16 supported.\n"
          ]
        }
      ],
      "source": [
        "capability = torch.cuda.get_device_capability()\n",
        "print(f\"CUDA device capability: {capability}\")\n",
        "\n",
        "# Tensor Cores are available on NVidia GPUs with CUDA >= 7 (e.g. Volta, Turing, Ampere, Hopper)\n",
        "if capability >= (7, 0):\n",
        "    print(\"Tensor Cores available: fast float16 supported.\")\n",
        "else:\n",
        "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "U4F4ZT43n1jq",
      "metadata": {
        "id": "U4F4ZT43n1jq"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "model.to(\"cuda\")\n",
        "model_half = deepcopy(model)\n",
        "model_half = model_half.half().to('cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "nfR0dyTfo4Zd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfR0dyTfo4Zd",
        "outputId": "baa9b2be-6817-4655-c9d5-f8a7836395e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[inference_forward] Inference time: 0.0125 seconds\n",
            "[inference_forward] Inference time: 0.0184 seconds\n",
            "[autocast_forward] Inference time: 0.0194 seconds\n"
          ]
        }
      ],
      "source": [
        "full_precision_time = inference_forward(inputs_gpu, model);\n",
        "half_precision_time = inference_forward(inputs_gpu, model_half);\n",
        "autocast_time = autocast_forward(inputs_gpu, model);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "EKvodQlSoPCT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKvodQlSoPCT",
        "outputId": "ddffd18b-74fd-41ec-fd7d-15a77da40958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[inference_forward] Inference time: 0.0127 seconds\n",
            "[inference_forward] Inference time: 0.0094 seconds\n",
            "[autocast_forward] Inference time: 0.0114 seconds\n"
          ]
        }
      ],
      "source": [
        "full_precision_time = inference_forward(inputs_gpu2, model);\n",
        "half_precision_time = inference_forward(inputs_gpu2, model_half);\n",
        "autocast_time = autocast_forward(inputs_gpu2, model);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52e10e60",
      "metadata": {},
      "source": [
        "In practice I would choose the fastest one, but before I would have to doublecheck that changing the precision does not affect model accuracy too much."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
