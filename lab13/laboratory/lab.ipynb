{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744ae44b",
   "metadata": {},
   "source": [
    "### Excercise 1\n",
    "\n",
    "Models comparison. First model without quantization was started with the following command.\n",
    "\n",
    "```bash\n",
    "vllm serve Qwen/Qwen3-1.7B \\\n",
    "  --port 8000 \\\n",
    "  --max-model-len 8192 \\\n",
    "  --gpu-memory-utilization 0.85\n",
    "```\n",
    "The Available KV cache memory for this option is 7.89 GiB, which results in GPU KV cache size of 73,824."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bccf664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "def benchmark_vllm_chat(\n",
    "    prompts,\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    model=\"\",\n",
    "    max_completion_tokens=200,\n",
    "):\n",
    "\n",
    "    client = OpenAI(api_key=\"EMPTY\", base_url=base_url)\n",
    "    start_time = time.perf_counter()\n",
    "    latencies = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        req_start = time.perf_counter()\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"developer\", \"content\": \"You are a helpful assistant. Answer with at least 200 tokens, for speed testing purposes\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            max_completion_tokens=max_completion_tokens,\n",
    "            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    "        )\n",
    "\n",
    "        _ = response.choices[0].message.content\n",
    "        latencies.append(time.perf_counter() - req_start)\n",
    "\n",
    "    total_time = time.perf_counter() - start_time\n",
    "\n",
    "    return {\n",
    "        \"num_requests\": len(prompts),\n",
    "        \"total_time_sec\": total_time,\n",
    "        \"avg_latency_sec\": sum(latencies) / len(latencies),\n",
    "        \"min_latency_sec\": min(latencies),\n",
    "        \"max_latency_sec\": max(latencies),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24de704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_requests': 10,\n",
       " 'total_time_sec': 26.040883401999963,\n",
       " 'avg_latency_sec': 2.604086875900066,\n",
       " 'min_latency_sec': 0.19839435099993352,\n",
       " 'max_latency_sec': 3.2785711090000405}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts = [\n",
    "    \"What kind of llm are you\",\n",
    "    \"How long do you think it takes LLMops lab\",\n",
    "    \"Explain KV cache\",\n",
    "    \"Explain flash attention\",\n",
    "    \"What is bitsandbytes?\",\n",
    "    \"What is vLLM?\",\n",
    "    \"What is the capital of Poland\",\n",
    "    \"Some random prompt for testing.\",\n",
    "    \"What is the answer to the ultimate question?\",\n",
    "    \"Have you seen Hitchhiker's guide to the galaxy?\"\n",
    "]\n",
    "\n",
    "\n",
    "benchmark_vllm_chat(test_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4639cf2",
   "metadata": {},
   "source": [
    "For the second one the quantization flag was added\n",
    "\n",
    "```bash\n",
    "vllm serve Qwen/Qwen3-1.7B \\\n",
    "  --port 8000 \\\n",
    "  --max-model-len 8192 \\\n",
    "  --gpu-memory-utilization 0.85 \\\n",
    "  --quantization bitsandbytes\n",
    "```\n",
    "\n",
    "The statistics were: \\\n",
    "Available KV cache memory: 9.77 GiB \\\n",
    "GPU KV cache size: 91,488 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0245f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_requests': 10,\n",
       " 'total_time_sec': 25.5037886099999,\n",
       " 'avg_latency_sec': 2.5503773868000734,\n",
       " 'min_latency_sec': 0.6125874009999279,\n",
       " 'max_latency_sec': 3.1932141540000885}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts = [\n",
    "    \"What kind of llm are you\",\n",
    "    \"How long do you think it takes LLMops lab\",\n",
    "    \"Explain KV cache\",\n",
    "    \"Explain flash attention\",\n",
    "    \"What is bitsandbytes?\",\n",
    "    \"What is vLLM?\",\n",
    "    \"What is the capital of Poland\",\n",
    "    \"Some random prompt for testing.\",\n",
    "    \"What is the answer to the ultimate question?\",\n",
    "    \"Have you seen Hitchhiker's guide to the galaxy?\"\n",
    "]\n",
    "\n",
    "\n",
    "benchmark_vllm_chat(test_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d0cdb",
   "metadata": {},
   "source": [
    "There is no visible, significant difference in latency. However, the min latency time is very small, I am not sure whether the clanker followed the system prompt correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a48e8e",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5a40a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-b03325044258f390', 'function': {'arguments': '{}', 'name': 'get_current_date'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': 'The current date is January 24, 2026. Two weeks from this date is January 31, 2026. \\n\\nNow, I will get the weather forecast for Birmingham on January 31, 2026.', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The current date is January 24, 2026. Two weeks from this date is January 31, 2026. \n",
      "\n",
      "Now, I will get the weather forecast for Birmingham on January 31, 2026.\n",
      "\n",
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-9f0effe96cc4b0d0', 'function': {'arguments': '{}', 'name': 'get_current_date'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-b64b678082612e08', 'function': {'arguments': '{\"country\": \"Poland\", \"city\": \"Warsaw\", \"date\": \"2026-01-25\"}', 'name': 'get_weather_forecast'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': 'The weather in Warsaw the day after tomorrow will be sunny.', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The weather in Warsaw the day after tomorrow will be sunny.\n",
      "\n",
      "Generated message: {'content': \"The weather in New York in two months will depend on the specific date you're interested in. Could you please specify the date?\", 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The weather in New York in two months will depend on the specific date you're interested in. Could you please specify the date?\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "from typing import Callable\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def make_llm_request(\n",
    "        prompt: str, \n",
    "        tool_definitions: list[dict], \n",
    "        tool_name_to_func: dict[str, Callable]\n",
    "    ) -> str:\n",
    "\n",
    "    client = OpenAI(api_key=\"EMPTY\", base_url=\"http://localhost:8000/v1\")\n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": \"You are a weather assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    # guard: loop limit, we break as soon as we get an answer\n",
    "    for _ in range(10):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"\",\n",
    "            messages=messages,\n",
    "            tools=tool_definitions,  # always pass all tools in this example\n",
    "            tool_choice=\"auto\",\n",
    "            max_completion_tokens=1000,\n",
    "            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    "        )\n",
    "        resp_message = response.choices[0].message\n",
    "        messages.append(resp_message.model_dump())\n",
    "\n",
    "        print(f\"Generated message: {resp_message.model_dump()}\")\n",
    "        print()\n",
    "\n",
    "        # parse possible tool calls (assume only \"function\" tools)\n",
    "        if resp_message.tool_calls:\n",
    "            for tool_call in resp_message.tool_calls:\n",
    "                func_name = tool_call.function.name\n",
    "                func_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "                # call tool, serialize result, append to messages\n",
    "                func = tool_name_to_func[func_name]\n",
    "                func_result = func(**func_args)\n",
    "\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": json.dumps(func_result),\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            # no tool calls, we're done\n",
    "            return resp_message.content\n",
    "\n",
    "    # we should not get here\n",
    "    last_response = resp_message.content\n",
    "    return f\"Could not resolve request, last response: {last_response}\"\n",
    "\n",
    "\n",
    "def get_tool_definitions() -> tuple[list[dict], dict[str, Callable]]:\n",
    "    tool_definitions = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_date\",\n",
    "                \"description\": 'Get current date in the format \"Year-Month-Day\" (YYYY-MM-DD).',\n",
    "                \"parameters\": {},\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather_forecast\",\n",
    "                \"description\": \"Get weather forecast at given country, city, and date.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"country\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The country the city is in.\",\n",
    "                        },\n",
    "                        \"city\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city to get the weather for.\",\n",
    "                        },\n",
    "                        \"date\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": (\n",
    "                                \"The date to get the weather for, \"\n",
    "                                'in the format \"Year-Month-Day\" (YYYY-MM-DD). '\n",
    "                                \"At most 4 weeks into the future.\"\n",
    "                            ),\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"country\", \"city\", \"date\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    tool_name_to_callable = {\n",
    "        \"get_current_date\": current_date_tool,\n",
    "        \"get_weather_forecast\": weather_forecast_tool,\n",
    "    }\n",
    "\n",
    "    return tool_definitions, tool_name_to_callable\n",
    "\n",
    "\n",
    "def current_date_tool() -> str:\n",
    "    return datetime.date.today().isoformat()\n",
    "\n",
    "\n",
    "def weather_forecast_tool(country: str, city: str, date: str) -> str:\n",
    "    if country.lower() in {\"united kingdom\", \"uk\", \"england\"}:\n",
    "        return \"Fog and rain\"\n",
    "    else:\n",
    "        return \"Sunshine\"\n",
    "\n",
    "\n",
    "prompt = \"What will be weather in Birmingham in two weeks?\"\n",
    "response = make_llm_request(prompt, *get_tool_definitions())\n",
    "print(\"Response:\\n\", response)\n",
    "\n",
    "print()\n",
    "\n",
    "prompt = \"What will be weather in Warsaw the day after tomorrow?\"\n",
    "response = make_llm_request(prompt, *get_tool_definitions())\n",
    "print(\"Response:\\n\", response)\n",
    "\n",
    "print()\n",
    "\n",
    "prompt = \"What will be weather in New York in two months?\"\n",
    "response = make_llm_request(prompt, *get_tool_definitions())\n",
    "print(\"Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11f9b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def get_dataset_tool_definitions() -> tuple[list[dict], dict]:\n",
    "    tool_definitions = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"read_remote_csv\",\n",
    "                \"description\": \"Read a CSV file from a URL and return the first n rows as text. n can be at most 20\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"url\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Public URL to a CSV file\",\n",
    "                        },\n",
    "                        \"n\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"Maximum number of rows to return\",\n",
    "                            \"default\": 50,\n",
    "                            \"minimum\": 0,\n",
    "                            \"maximum\": 20\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"url\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"read_remote_parquet\",\n",
    "                \"description\": \"Read a Parquet file from a URL and return the first n rows as text. n can be at most 20\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"url\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Public URL to a Parquet file\",\n",
    "                        },\n",
    "                        \"n\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"Maximum number of rows to return\",\n",
    "                            \"default\": 50,\n",
    "                            \"minimum\": 0,\n",
    "                            \"maximum\": 20\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"url\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    tool_name_to_callable = {\n",
    "        \"read_remote_csv\": read_remote_csv,\n",
    "        \"read_remote_parquet\": read_remote_parquet,\n",
    "    }\n",
    "\n",
    "    return tool_definitions, tool_name_to_callable\n",
    "\n",
    "def read_remote_csv(url: str, n: int) -> str:\n",
    "    n = min(n, 20)\n",
    "    try:\n",
    "        df = pl.read_csv(\n",
    "            url,\n",
    "            n_rows=n,\n",
    "            ignore_errors=True,\n",
    "        )\n",
    "        return str(df.to_dicts())\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR: Failed to read CSV from URL: {url}.\\n\"\n",
    "            f\"Reason: {type(e).__name__}: {e}\\n\"\n",
    "        )\n",
    "    \n",
    "\n",
    "def read_remote_parquet(url: str, n: int) -> str:\n",
    "    n = min(n, 20)\n",
    "    try:\n",
    "        df = pl.read_parquet(\n",
    "            url,\n",
    "            n_rows=n,\n",
    "        )\n",
    "        return str(df.to_dicts())\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR: Failed to read CSV from URL: {url}.\\n\"\n",
    "            f\"Reason: {type(e).__name__}: {e}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "572b54db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-a146d789c0d484f7', 'function': {'arguments': '{\"url\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet\", \"n\": 20}', 'name': 'read_remote_parquet'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': \"The average ride cost in NYC for January 2025 is calculated by summing up all the `total_amount` values from the first 20 rows of the dataset and then dividing by the number of rows. \\n\\nFrom the data provided:\\n\\n- The total amount for the first 20 rows is:\\n  $$\\n  18.0 + 12.12 + 12.1 + 9.7 + 8.3 + 24.1 + 11.75 + 19.1 + 27.1 + 16.4 + 16.4 + 12.96 + 19.2 + 12.9 + 38.9 + 22.7 + 25.55 - 8.54 - 8.54 + 12.2\\n  $$\\n\\nLet's calculate this sum:\\n\\n$$\\n18.0 + 12.12 = 30.12 \\\\\\\\\\n30.12 + 12.1 = 42.22 \\\\\\\\\\n42.22 + 12.1 = 54.32 \\\\\\\\\\n54.32 + 9.7 = 64.02 \\\\\\\\\\n64.02 + 8.3 = 72.32 \\\\\\\\\\n72.32 + 24.1 = 96.42 \\\\\\\\\\n96.42 + 11.75 = 108.17 \\\\\\\\\\n108.17 + 19.1 = 127.27 \\\\\\\\\\n127.27 + 27.1 = 154.37 \\\\\\\\\\n154.37 + 16.4 = 170.77 \\\\\\\\\\n170.77 + 16.4 = 187.17 \\\\\\\\\\n187.17 + 12.96 = 200.13 \\\\\\\\\\n200.13 + 19.2 = 219.33 \\\\\\\\\\n219.33 + 12.9 = 232.23 \\\\\\\\\\n232.23 + 38.9 = 271.13 \\\\\\\\\\n271.13 + 22.7 = 293.83 \\\\\\\\\\n293.83 + 25.55 = 319.38 \\\\\\\\\\n319.38 - 8.54 = 310.84 \\\\\\\\\\n310.84 - 8.54 = 302.30 \\\\\\\\\\n302.30 + 12.2 = 314.5\\n$$\\n\\nSo the total amount for the first 20 rows is $314.5.\\n\\nNow, dividing by 20 rows:\\n\\n$$\\n\\\\frac{314.5}{20} = 15.725\\n$$\\n\\nThus, the average ride cost in NYC for January 2025 is **$15.73**.\", 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The average ride cost in NYC for January 2025 is calculated by summing up all the `total_amount` values from the first 20 rows of the dataset and then dividing by the number of rows. \n",
      "\n",
      "From the data provided:\n",
      "\n",
      "- The total amount for the first 20 rows is:\n",
      "  $$\n",
      "  18.0 + 12.12 + 12.1 + 9.7 + 8.3 + 24.1 + 11.75 + 19.1 + 27.1 + 16.4 + 16.4 + 12.96 + 19.2 + 12.9 + 38.9 + 22.7 + 25.55 - 8.54 - 8.54 + 12.2\n",
      "  $$\n",
      "\n",
      "Let's calculate this sum:\n",
      "\n",
      "$$\n",
      "18.0 + 12.12 = 30.12 \\\\\n",
      "30.12 + 12.1 = 42.22 \\\\\n",
      "42.22 + 12.1 = 54.32 \\\\\n",
      "54.32 + 9.7 = 64.02 \\\\\n",
      "64.02 + 8.3 = 72.32 \\\\\n",
      "72.32 + 24.1 = 96.42 \\\\\n",
      "96.42 + 11.75 = 108.17 \\\\\n",
      "108.17 + 19.1 = 127.27 \\\\\n",
      "127.27 + 27.1 = 154.37 \\\\\n",
      "154.37 + 16.4 = 170.77 \\\\\n",
      "170.77 + 16.4 = 187.17 \\\\\n",
      "187.17 + 12.96 = 200.13 \\\\\n",
      "200.13 + 19.2 = 219.33 \\\\\n",
      "219.33 + 12.9 = 232.23 \\\\\n",
      "232.23 + 38.9 = 271.13 \\\\\n",
      "271.13 + 22.7 = 293.83 \\\\\n",
      "293.83 + 25.55 = 319.38 \\\\\n",
      "319.38 - 8.54 = 310.84 \\\\\n",
      "310.84 - 8.54 = 302.30 \\\\\n",
      "302.30 + 12.2 = 314.5\n",
      "$$\n",
      "\n",
      "So the total amount for the first 20 rows is $314.5.\n",
      "\n",
      "Now, dividing by 20 rows:\n",
      "\n",
      "$$\n",
      "\\frac{314.5}{20} = 15.725\n",
      "$$\n",
      "\n",
      "Thus, the average ride cost in NYC for January 2025 is **$15.73**.\n"
     ]
    }
   ],
   "source": [
    "apis_tox_url = \"https://raw.githubusercontent.com/j-adamczyk/ApisTox_dataset/master/outputs/dataset_final.csv\"\n",
    "taxi_data_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet\"\n",
    "\n",
    "prompt = (\n",
    "    f\"Here you have a daset on taxi rides: {taxi_data_url} \"\n",
    "    f\"How much did the average ride cost in NYC taxi cost in January 2025\"\n",
    ")\n",
    "\n",
    "response = make_llm_request(prompt, *get_dataset_tool_definitions())\n",
    "print(\"Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd3e72a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-81d653b2b0ad1d2b', 'function': {'arguments': '{\"url\": \"https://raw.githubusercontent.com/j-adamczyk/ApisTox_dataset/master/outputs/dataset_final.csv\", \"n\": 20}', 'name': 'read_remote_csv'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': 'The longest molecule name in the dataset is **\"S-[(1,3-Dihydro-1,3-dioxo-2H-isoindol-2-yl)methyl]O,O-dimethyl ester, Phosphorodithioic acid\"**, which has 48 characters.', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The longest molecule name in the dataset is **\"S-[(1,3-Dihydro-1,3-dioxo-2H-isoindol-2-yl)methyl]O,O-dimethyl ester, Phosphorodithioic acid\"**, which has 48 characters.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    f\"Here you have an ApixTox dataset: {apis_tox_url}. \"\n",
    "    f\"What is the longest molecule name you see in the dataset?\"\n",
    ")\n",
    "\n",
    "response = make_llm_request(prompt, *get_dataset_tool_definitions())\n",
    "print(\"Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d2876",
   "metadata": {},
   "source": [
    "## Excercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc41eaa0",
   "metadata": {},
   "source": [
    "Server implementation -> [datetime_mcp_serve](./mcp_servers/datetime.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd1572",
   "metadata": {},
   "source": [
    "## Excercise 4\n",
    "\n",
    "Server implementation -> [visualisation_mcp_srver](./mcp_servers/visualisation.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aef0f2",
   "metadata": {},
   "source": [
    "### Excercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26227fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails import Guard, OnFailAction\n",
    "from guardrails.hub import RestrictToTopic, DetectJailbreak\n",
    "from openai import OpenAI\n",
    "\n",
    "def fishing_fanatic(prompt: str) -> str:\n",
    "    client = OpenAI(api_key=\"EMPTY\", base_url=\"http://localhost:8000/v1\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a fishing fanatic. Only talk about fishing.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"\",\n",
    "        messages=messages,\n",
    "        max_completion_tokens=300,\n",
    "        extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    guard = (\n",
    "        Guard()\n",
    "        .use(\n",
    "            RestrictToTopic,\n",
    "            valid_topics=[\"fishing, fish, sea life\"],\n",
    "            on_fail=OnFailAction.EXCEPTION,\n",
    "        )\n",
    "        .use(\n",
    "            DetectJailbreak,\n",
    "            on_fail=OnFailAction.EXCEPTION,\n",
    "        )\n",
    "    )\n",
    "    print(f\"Before validation: {content}\")\n",
    "    try:\n",
    "        guard.validate(content)\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Salmon\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8629cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before validation: Ah, a great question! üé£Ô∏è For a fishing-themed dinner, how about something hearty and satisfying? How about **fishing chow**? It's a special dish made with fish, potatoes, and a bit of cheese, often served with a side of bread. Or, if you're into something more adventurous, maybe **fishing fries** ‚Äî crispy fried fish with a tangy sauce. \n",
      "\n",
      "But wait, I should ask ‚Äî what kind of fish do you like? Salmon, trout, bass, or something more exotic? And do you want it spicy, sweet, or savory? Let me know, and I'll tailor it just for you! üêü\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/MLOPS/lab13/laboratory/.venv/lib/python3.11/site-packages/guardrails/validator_service/__init__.py:84: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salmon\n"
     ]
    }
   ],
   "source": [
    "print(fishing_fanatic(\"What shoud I eat today for dinner?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9135a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before validation: Ah, the question is intriguing! As a fishing fanatic, I must say I'm partial to the **striped bass**. It's a fish that dances in the water like a silver ribbon, and its flavor is simply unmatched. The way it bites, the way it fights, and the way it tastes‚Äîoh, it's a true treasure. Plus, there's something magical about the way it reflects the sunlight, like a piece of art. üé®üêü\n",
      "Salmon\n"
     ]
    }
   ],
   "source": [
    "print(fishing_fanatic(\"What is your favourite fish and why?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aabc10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before validation: Oh, fish! They're the most fascinating creatures in the ocean, right? Every type of fish has its own unique traits, from the colorful ones that swim in the clear waters to the deep-sea dwellers that live in the darkest parts of the ocean. I love how they come in all shapes and sizes‚Äîsome are big and powerful, others are small and delicate. The way they move, the way they swim, the way they eat... it's all so beautiful. I always feel so lucky to be able to see them in their natural habitat. Do you like fish? What kind do you prefer?\n",
      "Salmon\n"
     ]
    }
   ],
   "source": [
    "print(fishing_fanatic(\"Just talk about fish\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-course-agh-lab-13 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
