{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744ae44b",
   "metadata": {},
   "source": [
    "## Excercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af831aa1",
   "metadata": {},
   "source": [
    "```bash\n",
    "vllm serve Qwen/Qwen3-1.7B \\\n",
    "  --port 8000 \\\n",
    "  --max-model-len 8192 \\\n",
    "  --max-num-seqs 32 \\\n",
    "  --gpu-memory-utilization 0.85\n",
    "\n",
    "vllm serve Qwen/Qwen3-1.7B \\\n",
    "  --port 8000 \\\n",
    "  --max-model-len 8192 \\\n",
    "  --max-num-seqs 32 \\\n",
    "  --gpu-memory-utilization 0.85 \\\n",
    "  --quantization bitsandbytes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8e5d2",
   "metadata": {},
   "source": [
    "(EngineCore_DP0 pid=29361) INFO 01-21 16:28:44 [monitor.py:34] torch.compile takes 12.04 s in total\n",
    "(EngineCore_DP0 pid=29361) INFO 01-21 16:28:45 [gpu_worker.py:358] Available KV cache memory: 9.04 GiB\n",
    "(EngineCore_DP0 pid=29361) INFO 01-21 16:28:46 [kv_cache_utils.py:1305] GPU KV cache size: 84,608 tokens\n",
    "(EngineCore_DP0 pid=29361) INFO 01-21 16:28:46 [kv_cache_utils.py:1310] Maximum concurrency for 8,192 tokens per request: 10.33x\n",
    "\n",
    "\n",
    "(EngineCore_DP0 pid=34017) INFO 01-21 16:40:15 [gpu_worker.py:358] Available KV cache memory: 10.57 GiB\n",
    "(EngineCore_DP0 pid=34017) INFO 01-21 16:40:16 [kv_cache_utils.py:1305] GPU KV cache size: 98,976 tokens\n",
    "(EngineCore_DP0 pid=34017) INFO 01-21 16:40:16 [kv_cache_utils.py:1310] Maximum concurrency for 8,192 tokens per request: 12.08x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7c5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "def benchmark_vllm_chat(\n",
    "    prompts,\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    model=\"\",\n",
    "    max_completion_tokens=200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Benchmarks vLLM OpenAI-compatible chat completions.\n",
    "\n",
    "    Args:\n",
    "        prompts (list[str]): List of user prompts.\n",
    "        base_url (str): vLLM server URL.\n",
    "        model (str): Model name (empty string = default server model).\n",
    "        max_completion_tokens (int): Max tokens per response.\n",
    "\n",
    "    Returns:\n",
    "        dict with timing statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    client = OpenAI(api_key=\"EMPTY\", base_url=base_url)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    latencies = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        req_start = time.perf_counter()\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            max_completion_tokens=max_completion_tokens,\n",
    "            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    "        )\n",
    "\n",
    "        _ = response.choices[0].message.content\n",
    "        latencies.append(time.perf_counter() - req_start)\n",
    "\n",
    "    total_time = time.perf_counter() - start_time\n",
    "\n",
    "    return {\n",
    "        \"num_requests\": len(prompts),\n",
    "        \"total_time_sec\": total_time,\n",
    "        \"avg_latency_sec\": sum(latencies) / len(latencies),\n",
    "        \"min_latency_sec\": min(latencies),\n",
    "        \"max_latency_sec\": max(latencies),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12b6274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_requests': 10,\n",
       " 'total_time_sec': 32.64301064999995,\n",
       " 'avg_latency_sec': 3.2642996998999934,\n",
       " 'min_latency_sec': 3.238822791000075,\n",
       " 'max_latency_sec': 3.418850865000195}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Explain what MLOps is.\",\n",
    "    \"What is LLMOps?\",\n",
    "    \"Explain KV cache in transformers.\",\n",
    "    \"What is dynamic batching?\",\n",
    "    \"Explain model quantization.\",\n",
    "    \"What is bitsandbytes?\",\n",
    "    \"What is FlashAttention?\",\n",
    "    \"Explain prefix caching.\",\n",
    "    \"What is vLLM?\",\n",
    "    \"Difference between FP16 and INT4.\",\n",
    "]\n",
    "\n",
    "results = benchmark_vllm_chat(prompts)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa6936",
   "metadata": {},
   "source": [
    "# TODO task 1: benchmark other quanitzaziton change benchamrking code, change prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a48e8e",
   "metadata": {},
   "source": [
    "## Exercise 2 (tool calling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6e99f",
   "metadata": {},
   "source": [
    "vllm serve Qwen/Qwen3-1.7B \\\n",
    "  --port 8000 \\\n",
    "  --max-model-len 8192 \\\n",
    "  --max-num-seqs 32 \\\n",
    "  --gpu-memory-utilization 0.85 \\\n",
    "  --enable-auto-tool-choice \\\n",
    "  --tool-call-parser hermes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a40a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-a18fd8a03389e6d5', 'function': {'arguments': '{}', 'name': 'get_current_date'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': 'The current date is January 23, 2026. The weather in Birmingham in two weeks (which is 14 days from January 23, 2026) will be available once I have the weather forecast for that date. Let me get the weather forecast for Birmingham in two weeks.', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The current date is January 23, 2026. The weather in Birmingham in two weeks (which is 14 days from January 23, 2026) will be available once I have the weather forecast for that date. Let me get the weather forecast for Birmingham in two weeks.\n",
      "\n",
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-ab655c7906075e61', 'function': {'arguments': '{}', 'name': 'get_current_date'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-985e67034aa5e1b2', 'function': {'arguments': '{\"country\": \"Poland\", \"city\": \"Warsaw\", \"date\": \"2026-01-24\"}', 'name': 'get_weather_forecast'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': 'The weather in Warsaw the day after tomorrow will be sunny.', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The weather in Warsaw the day after tomorrow will be sunny.\n",
      "\n",
      "Generated message: {'content': 'The weather in New York in two months will be determined by the weather forecast service. Let me check the weather forecast for New York in two months.\\n\\n', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-969acecbb8bed10d', 'function': {'arguments': '{\"country\": \"United States\", \"city\": \"New York\", \"date\": \"2023-10-01\"}', 'name': 'get_weather_forecast'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': 'The weather in New York in two months will be sunny.', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The weather in New York in two months will be sunny.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "from typing import Callable\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def make_llm_request(\n",
    "        prompt: str, \n",
    "        tool_definitions: list[dict], \n",
    "        tool_name_to_func: dict[str, Callable]\n",
    "    ) -> str:\n",
    "\n",
    "    client = OpenAI(api_key=\"EMPTY\", base_url=\"http://localhost:8000/v1\")\n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": \"You are a weather assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    # guard: loop limit, we break as soon as we get an answer\n",
    "    for _ in range(10):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"\",\n",
    "            messages=messages,\n",
    "            tools=tool_definitions,  # always pass all tools in this example\n",
    "            tool_choice=\"auto\",\n",
    "            max_completion_tokens=1000,\n",
    "            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    "        )\n",
    "        resp_message = response.choices[0].message\n",
    "        messages.append(resp_message.model_dump())\n",
    "\n",
    "        print(f\"Generated message: {resp_message.model_dump()}\")\n",
    "        print()\n",
    "\n",
    "        # parse possible tool calls (assume only \"function\" tools)\n",
    "        if resp_message.tool_calls:\n",
    "            for tool_call in resp_message.tool_calls:\n",
    "                func_name = tool_call.function.name\n",
    "                func_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "                # call tool, serialize result, append to messages\n",
    "                func = tool_name_to_func[func_name]\n",
    "                func_result = func(**func_args)\n",
    "\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": json.dumps(func_result),\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            # no tool calls, we're done\n",
    "            return resp_message.content\n",
    "\n",
    "    # we should not get here\n",
    "    last_response = resp_message.content\n",
    "    return f\"Could not resolve request, last response: {last_response}\"\n",
    "\n",
    "\n",
    "def get_tool_definitions() -> tuple[list[dict], dict[str, Callable]]:\n",
    "    tool_definitions = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_date\",\n",
    "                \"description\": 'Get current date in the format \"Year-Month-Day\" (YYYY-MM-DD).',\n",
    "                \"parameters\": {},\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather_forecast\",\n",
    "                \"description\": \"Get weather forecast at given country, city, and date.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"country\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The country the city is in.\",\n",
    "                        },\n",
    "                        \"city\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city to get the weather for.\",\n",
    "                        },\n",
    "                        \"date\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": (\n",
    "                                \"The date to get the weather for, \"\n",
    "                                'in the format \"Year-Month-Day\" (YYYY-MM-DD). '\n",
    "                                \"At most 4 weeks into the future.\"\n",
    "                            ),\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"country\", \"city\", \"date\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    tool_name_to_callable = {\n",
    "        \"get_current_date\": current_date_tool,\n",
    "        \"get_weather_forecast\": weather_forecast_tool,\n",
    "    }\n",
    "\n",
    "    return tool_definitions, tool_name_to_callable\n",
    "\n",
    "\n",
    "def current_date_tool() -> str:\n",
    "    return datetime.date.today().isoformat()\n",
    "\n",
    "\n",
    "def weather_forecast_tool(country: str, city: str, date: str) -> str:\n",
    "    if country.lower() in {\"united kingdom\", \"uk\", \"england\"}:\n",
    "        return \"Fog and rain\"\n",
    "    else:\n",
    "        return \"Sunshine\"\n",
    "\n",
    "\n",
    "prompt = \"What will be weather in Birmingham in two weeks?\"\n",
    "response = make_llm_request(prompt, *get_tool_definitions())\n",
    "print(\"Response:\\n\", response)\n",
    "\n",
    "print()\n",
    "\n",
    "prompt = \"What will be weather in Warsaw the day after tomorrow?\"\n",
    "response = make_llm_request(prompt, *get_tool_definitions())\n",
    "print(\"Response:\\n\", response)\n",
    "\n",
    "print()\n",
    "\n",
    "prompt = \"What will be weather in New York in two months?\"\n",
    "response = make_llm_request(prompt, *get_tool_definitions())\n",
    "print(\"Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f9b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def get_dataset_tool_definitions() -> tuple[list[dict], dict]:\n",
    "    tool_definitions = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"read_remote_csv\",\n",
    "                \"description\": \"Read a CSV file from a URL and return the first n rows as text. n can be at most 20\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"url\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Public URL to a CSV file\",\n",
    "                        },\n",
    "                        \"n\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"Maximum number of rows to return\",\n",
    "                            \"default\": 50,\n",
    "                            \"minimum\": 0,\n",
    "                            \"maximum\": 20\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"url\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"read_remote_parquet\",\n",
    "                \"description\": \"Read a Parquet file from a URL and return the first n rows as text. n can be at most 20\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"url\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Public URL to a Parquet file\",\n",
    "                        },\n",
    "                        \"n\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"Maximum number of rows to return\",\n",
    "                            \"default\": 50,\n",
    "                            \"minimum\": 0,\n",
    "                            \"maximum\": 20\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"url\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    tool_name_to_callable = {\n",
    "        \"read_remote_csv\": read_remote_csv,\n",
    "        \"read_remote_parquet\": read_remote_parquet,\n",
    "    }\n",
    "\n",
    "    return tool_definitions, tool_name_to_callable\n",
    "\n",
    "def read_remote_csv(url: str, n: int) -> str:\n",
    "    n = min(n, 20)\n",
    "    try:\n",
    "        df = pl.read_csv(\n",
    "            url,\n",
    "            n_rows=n,\n",
    "            ignore_errors=True,\n",
    "        )\n",
    "        return str(df.to_dicts())\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR: Failed to read CSV from URL: {url}.\\n\"\n",
    "            f\"Reason: {type(e).__name__}: {e}\\n\"\n",
    "        )\n",
    "    \n",
    "\n",
    "def read_remote_parquet(url: str, n: int) -> str:\n",
    "    n = min(n, 20)\n",
    "    try:\n",
    "        df = pl.read_parquet(\n",
    "            url,\n",
    "            n_rows=n,\n",
    "        )\n",
    "        return str(df.to_dicts())\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR: Failed to read CSV from URL: {url}.\\n\"\n",
    "            f\"Reason: {type(e).__name__}: {e}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572b54db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-a33efe17b07904ea', 'function': {'arguments': '{\"url\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet\", \"n\": 20}', 'name': 'read_remote_parquet'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': \"The average ride cost in NYC for January 2025 is calculated by summing up all the `total_amount` values from the first 20 rows of the dataset and then dividing by 20. Let's calculate that.\\n\\n### Step-by-step Calculation:\\n1. **Sum of total_amount:**\\n   $$\\n   18.0 + 12.12 + 12.1 + 9.7 + 8.3 + 24.1 + 11.75 + 19.1 + 27.1 + 16.4 + 16.4 + 12.96 + 19.2 + 12.9 + 38.9 + 22.7 + 25.55 - 8.54 - 8.54 + 12.2 + 20.6\\n   $$\\n\\n   Adding these values:\\n   $$\\n   18.0 + 12.12 = 30.12 \\\\\\\\\\n   30.12 + 12.1 = 42.22 \\\\\\\\\\n   42.22 + 12.1 = 54.32 \\\\\\\\\\n   54.32 + 9.7 = 64.02 \\\\\\\\\\n   64.02 + 8.3 = 72.32 \\\\\\\\\\n   72.32 + 24.1 = 96.42 \\\\\\\\\\n   96.42 + 11.75 = 108.17 \\\\\\\\\\n   108.17 + 19.1 = 127.27 \\\\\\\\\\n   127.27 + 27.1 = 154.37 \\\\\\\\\\n   154.37 + 16.4 = 170.77 \\\\\\\\\\n   170.77 + 16.4 = 187.17 \\\\\\\\\\n   187.17 + 12.96 = 200.13 \\\\\\\\\\n   200.13 + 19.2 = 219.33 \\\\\\\\\\n   219.33 + 12.9 = 232.23 \\\\\\\\\\n   232.23 + 38.9 = 271.13 \\\\\\\\\\n   271.13 + 22.7 = 293.83 \\\\\\\\\\n   293.83 + 25.55 = 319.38 \\\\\\\\\\n   319.38 - 8.54 = 310.84 \\\\\\\\\\n   310.84 - 8.54 = 302.30 \\\\\\\\\\n   302.30 + 12.2 = 314.5 \\\\\\\\\\n   314.5 + 20.6 = 335.1\\n   $$\\n\\n2. **Average ride cost:**\\n   $$\\n   \\\\frac{335.1}{20} = 16.755\\n   $$\\n\\n### Final Answer:\\nThe average ride cost in NYC taxi for January 2025 was **$16.75**.\", 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The average ride cost in NYC for January 2025 is calculated by summing up all the `total_amount` values from the first 20 rows of the dataset and then dividing by 20. Let's calculate that.\n",
      "\n",
      "### Step-by-step Calculation:\n",
      "1. **Sum of total_amount:**\n",
      "   $$\n",
      "   18.0 + 12.12 + 12.1 + 9.7 + 8.3 + 24.1 + 11.75 + 19.1 + 27.1 + 16.4 + 16.4 + 12.96 + 19.2 + 12.9 + 38.9 + 22.7 + 25.55 - 8.54 - 8.54 + 12.2 + 20.6\n",
      "   $$\n",
      "\n",
      "   Adding these values:\n",
      "   $$\n",
      "   18.0 + 12.12 = 30.12 \\\\\n",
      "   30.12 + 12.1 = 42.22 \\\\\n",
      "   42.22 + 12.1 = 54.32 \\\\\n",
      "   54.32 + 9.7 = 64.02 \\\\\n",
      "   64.02 + 8.3 = 72.32 \\\\\n",
      "   72.32 + 24.1 = 96.42 \\\\\n",
      "   96.42 + 11.75 = 108.17 \\\\\n",
      "   108.17 + 19.1 = 127.27 \\\\\n",
      "   127.27 + 27.1 = 154.37 \\\\\n",
      "   154.37 + 16.4 = 170.77 \\\\\n",
      "   170.77 + 16.4 = 187.17 \\\\\n",
      "   187.17 + 12.96 = 200.13 \\\\\n",
      "   200.13 + 19.2 = 219.33 \\\\\n",
      "   219.33 + 12.9 = 232.23 \\\\\n",
      "   232.23 + 38.9 = 271.13 \\\\\n",
      "   271.13 + 22.7 = 293.83 \\\\\n",
      "   293.83 + 25.55 = 319.38 \\\\\n",
      "   319.38 - 8.54 = 310.84 \\\\\n",
      "   310.84 - 8.54 = 302.30 \\\\\n",
      "   302.30 + 12.2 = 314.5 \\\\\n",
      "   314.5 + 20.6 = 335.1\n",
      "   $$\n",
      "\n",
      "2. **Average ride cost:**\n",
      "   $$\n",
      "   \\frac{335.1}{20} = 16.755\n",
      "   $$\n",
      "\n",
      "### Final Answer:\n",
      "The average ride cost in NYC taxi for January 2025 was **$16.75**.\n"
     ]
    }
   ],
   "source": [
    "apis_tox_url = \"https://raw.githubusercontent.com/j-adamczyk/ApisTox_dataset/master/outputs/dataset_final.csv\"\n",
    "taxi_data_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet\"\n",
    "\n",
    "\n",
    "prompt = (\n",
    "    f\"Here you have a daset on taxi rides: {taxi_data_url} \"\n",
    "    f\"How much did the average ride cost in NYC taxi cost in January 2025\"\n",
    ")\n",
    "\n",
    "response = make_llm_request(prompt, *get_dataset_tool_definitions())\n",
    "print(\"Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d2876",
   "metadata": {},
   "source": [
    "## Excercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd1572",
   "metadata": {},
   "source": [
    "## Excercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aef0f2",
   "metadata": {},
   "source": [
    "## Excercise 5\n",
    "\n",
    "I am unable to generate api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26227fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not pass guardrail, fixing\n",
      "Response:\n",
      " I recommend a fresh catch of fish, whether it be salmon, trout, or halibut, for a delicious and nutritious dinner.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def check_output_guardrail_competitor_mention(client: OpenAI, prompt: str) -> bool:\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=\"\",  # use the default server model\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"developer\",\n",
    "                \"content\": \"You are a old fishing fanatic, focusing on fish exclusively, talking only about fish.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Does the following text mention any food other than fish quite positively? \"\n",
    "                    f\"Output ONLY 0 (no mention) or 1 (mention).\\n{prompt}\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "        max_completion_tokens=1000,\n",
    "        extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    "    )\n",
    "    content = chat_response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        return int(content) == 0  # pass if we don't detect any problem\n",
    "    except ValueError:\n",
    "        return True  # passes by default\n",
    "\n",
    "\n",
    "def make_llm_request(prompt: str) -> str:\n",
    "    client = OpenAI(api_key=\"EMPTY\", base_url=\"http://localhost:8000/v1\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You are a old fishing fanatic, focusing on fish exclusively, talking only about fish.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=\"\",  # use the default server model\n",
    "        messages=messages,\n",
    "        max_completion_tokens=1000,\n",
    "        extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    "    )\n",
    "    content = chat_response.choices[0].message.content.strip()\n",
    "\n",
    "    passed_guardrail = check_output_guardrail_competitor_mention(client, content)\n",
    "    if not passed_guardrail:\n",
    "        print(\"Did not pass guardrail, fixing\")\n",
    "        messages += [\n",
    "            {\"role\": \"assistant\", \"content\": content},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Previous text contained mention of something other than fish, fix that. \"\n",
    "                \"Output only the new fishing fanatic recommendation, without clearly showing any bias. \"\n",
    "                \"No additional comments, acknowledgements etc.\",\n",
    "            },\n",
    "        ]\n",
    "        chat_response = client.chat.completions.create(\n",
    "            model=\"\",  # use the default server model\n",
    "            messages=messages,\n",
    "            max_completion_tokens=1000,\n",
    "            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    "        )\n",
    "        content = chat_response.choices[0].message.content.strip()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"What should I have for dinner today?\"\n",
    "response = make_llm_request(prompt)\n",
    "print(\"Response:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-course-agh-lab-13 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
