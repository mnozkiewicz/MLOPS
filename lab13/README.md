# Lab 13 - LLMOps

This lab concerns LLMOps, operationalizing LLMs for inference. We will cover
efficient inference, tool usage, and securing LLMs with guardrails.

**Learning plan**
1. Efficient LLM inference
   - traditional HuggingFace inference
   - vLLM
2. Tool usage
   - manual function calling
   - Model Context Protocol (MCP)
3. LLM security
   - guardrails
   - Guardrails AI framework

**Necessary software**
- [Docker and Docker Compose](https://docs.docker.com/engine/install/), 
  also [see those post-installation notes](https://docs.docker.com/engine/install/linux-postinstall/)
- [uv](https://docs.astral.sh/uv/getting-started/installation/)

Note that you should also activate `uv` project and install dependencies with `uv sync`.

**Lab**

See [lab instruction](LAB_INSTRUCTION.md). Laboratory is worth 5 points.

**Homework**

See [homework instruction](HOMEWORK.md). Homework is worth 10 points.
