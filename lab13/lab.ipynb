{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4baacb68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "744ae44b",
   "metadata": {},
   "source": [
    "## Excercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af831aa1",
   "metadata": {},
   "source": [
    "```bash\n",
    "vllm serve Qwen/Qwen3-1.7B \\\n",
    "  --port 8000 \\\n",
    "  --max-model-len 8192 \\\n",
    "  --max-num-seqs 32 \\\n",
    "  --gpu-memory-utilization 0.85\n",
    "\n",
    "vllm serve Qwen/Qwen3-1.7B \\\n",
    "  --port 8000 \\\n",
    "  --max-model-len 8192 \\\n",
    "  --max-num-seqs 32 \\\n",
    "  --gpu-memory-utilization 0.85 \\\n",
    "  --quantization bitsandbytes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8e5d2",
   "metadata": {},
   "source": [
    "(EngineCore_DP0 pid=29361) INFO 01-21 16:28:44 [monitor.py:34] torch.compile takes 12.04 s in total\n",
    "(EngineCore_DP0 pid=29361) INFO 01-21 16:28:45 [gpu_worker.py:358] Available KV cache memory: 9.04 GiB\n",
    "(EngineCore_DP0 pid=29361) INFO 01-21 16:28:46 [kv_cache_utils.py:1305] GPU KV cache size: 84,608 tokens\n",
    "(EngineCore_DP0 pid=29361) INFO 01-21 16:28:46 [kv_cache_utils.py:1310] Maximum concurrency for 8,192 tokens per request: 10.33x\n",
    "\n",
    "\n",
    "(EngineCore_DP0 pid=34017) INFO 01-21 16:40:15 [gpu_worker.py:358] Available KV cache memory: 10.57 GiB\n",
    "(EngineCore_DP0 pid=34017) INFO 01-21 16:40:16 [kv_cache_utils.py:1305] GPU KV cache size: 98,976 tokens\n",
    "(EngineCore_DP0 pid=34017) INFO 01-21 16:40:16 [kv_cache_utils.py:1310] Maximum concurrency for 8,192 tokens per request: 12.08x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7c5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "def benchmark_vllm_chat(\n",
    "    prompts,\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    model=\"\",\n",
    "    max_completion_tokens=200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Benchmarks vLLM OpenAI-compatible chat completions.\n",
    "\n",
    "    Args:\n",
    "        prompts (list[str]): List of user prompts.\n",
    "        base_url (str): vLLM server URL.\n",
    "        model (str): Model name (empty string = default server model).\n",
    "        max_completion_tokens (int): Max tokens per response.\n",
    "\n",
    "    Returns:\n",
    "        dict with timing statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    client = OpenAI(api_key=\"EMPTY\", base_url=base_url)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    latencies = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        req_start = time.perf_counter()\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            max_completion_tokens=max_completion_tokens,\n",
    "            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    "        )\n",
    "\n",
    "        _ = response.choices[0].message.content\n",
    "        latencies.append(time.perf_counter() - req_start)\n",
    "\n",
    "    total_time = time.perf_counter() - start_time\n",
    "\n",
    "    return {\n",
    "        \"num_requests\": len(prompts),\n",
    "        \"total_time_sec\": total_time,\n",
    "        \"avg_latency_sec\": sum(latencies) / len(latencies),\n",
    "        \"min_latency_sec\": min(latencies),\n",
    "        \"max_latency_sec\": max(latencies),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12b6274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_requests': 10,\n",
       " 'total_time_sec': 31.157270540000354,\n",
       " 'avg_latency_sec': 3.1157256762000087,\n",
       " 'min_latency_sec': 3.0992684600000757,\n",
       " 'max_latency_sec': 3.1314260409999406}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Explain what MLOps is.\",\n",
    "    \"What is LLMOps?\",\n",
    "    \"Explain KV cache in transformers.\",\n",
    "    \"What is dynamic batching?\",\n",
    "    \"Explain model quantization.\",\n",
    "    \"What is bitsandbytes?\",\n",
    "    \"What is FlashAttention?\",\n",
    "    \"Explain prefix caching.\",\n",
    "    \"What is vLLM?\",\n",
    "    \"Difference between FP16 and INT4.\",\n",
    "]\n",
    "\n",
    "results = benchmark_vllm_chat(prompts)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa6936",
   "metadata": {},
   "source": [
    "# TODO task 1: benchmark other quanitzaziton change benchamrking code, change prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a48e8e",
   "metadata": {},
   "source": [
    "## Exercise 2 (tool calling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6e99f",
   "metadata": {},
   "source": [
    "vllm serve Qwen/Qwen3-1.7B \\\n",
    "  --port 8000 \\\n",
    "  --max-model-len 8192 \\\n",
    "  --max-num-seqs 32 \\\n",
    "  --gpu-memory-utilization 0.85 \\\n",
    "  --enable-auto-tool-choice \\\n",
    "  --tool-call-parser hermes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5a40a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-a4ae362e79ab9155', 'function': {'arguments': '{}', 'name': 'get_current_date'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-91c115de7ffa4728', 'function': {'arguments': '{\"country\": \"United Kingdom\", \"city\": \"Birmingham\", \"date\": \"2026-01-21\"}', 'name': 'get_weather_forecast'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': 'The weather in Birmingham, United Kingdom, on January 21, 2026, is expected to be fog and rain.', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The weather in Birmingham, United Kingdom, on January 21, 2026, is expected to be fog and rain.\n",
      "\n",
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-baf3fee37b8df136', 'function': {'arguments': '{}', 'name': 'get_current_date'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-86d13addca6bcd57', 'function': {'arguments': '{\"country\": \"Poland\", \"city\": \"Warsaw\", \"date\": \"2026-01-22\"}', 'name': 'get_weather_forecast'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': 'The weather in Warsaw the day after tomorrow will be sunny.', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The weather in Warsaw the day after tomorrow will be sunny.\n",
      "\n",
      "Generated message: {'content': 'The weather in New York in two months will depend on the specific date and conditions. I can provide the weather forecast for a specific date if you provide the exact date. Please provide the date in the format \"Year-Month-Day\" (YYYY-MM-DD).', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The weather in New York in two months will depend on the specific date and conditions. I can provide the weather forecast for a specific date if you provide the exact date. Please provide the date in the format \"Year-Month-Day\" (YYYY-MM-DD).\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "from typing import Callable\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def make_llm_request(\n",
    "        prompt: str, \n",
    "        tool_definitions: list[dict], \n",
    "        tool_name_to_func: dict[str, Callable]\n",
    "    ) -> str:\n",
    "\n",
    "    client = OpenAI(api_key=\"EMPTY\", base_url=\"http://localhost:8000/v1\")\n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": \"You are a weather assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    # guard: loop limit, we break as soon as we get an answer\n",
    "    for _ in range(10):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"\",\n",
    "            messages=messages,\n",
    "            tools=tool_definitions,  # always pass all tools in this example\n",
    "            tool_choice=\"auto\",\n",
    "            max_completion_tokens=1000,\n",
    "            extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    "        )\n",
    "        resp_message = response.choices[0].message\n",
    "        messages.append(resp_message.model_dump())\n",
    "\n",
    "        print(f\"Generated message: {resp_message.model_dump()}\")\n",
    "        print()\n",
    "\n",
    "        # parse possible tool calls (assume only \"function\" tools)\n",
    "        if resp_message.tool_calls:\n",
    "            for tool_call in resp_message.tool_calls:\n",
    "                func_name = tool_call.function.name\n",
    "                func_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "                # call tool, serialize result, append to messages\n",
    "                func = tool_name_to_func[func_name]\n",
    "                func_result = func(**func_args)\n",
    "\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": json.dumps(func_result),\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            # no tool calls, we're done\n",
    "            return resp_message.content\n",
    "\n",
    "    # we should not get here\n",
    "    last_response = resp_message.content\n",
    "    return f\"Could not resolve request, last response: {last_response}\"\n",
    "\n",
    "\n",
    "def get_tool_definitions() -> tuple[list[dict], dict[str, Callable]]:\n",
    "    tool_definitions = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_date\",\n",
    "                \"description\": 'Get current date in the format \"Year-Month-Day\" (YYYY-MM-DD).',\n",
    "                \"parameters\": {},\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather_forecast\",\n",
    "                \"description\": \"Get weather forecast at given country, city, and date.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"country\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The country the city is in.\",\n",
    "                        },\n",
    "                        \"city\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city to get the weather for.\",\n",
    "                        },\n",
    "                        \"date\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": (\n",
    "                                \"The date to get the weather for, \"\n",
    "                                'in the format \"Year-Month-Day\" (YYYY-MM-DD). '\n",
    "                                \"At most 4 weeks into the future.\"\n",
    "                            ),\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"country\", \"city\", \"date\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    tool_name_to_callable = {\n",
    "        \"get_current_date\": current_date_tool,\n",
    "        \"get_weather_forecast\": weather_forecast_tool,\n",
    "    }\n",
    "\n",
    "    return tool_definitions, tool_name_to_callable\n",
    "\n",
    "\n",
    "def current_date_tool() -> str:\n",
    "    return datetime.date.today().isoformat()\n",
    "\n",
    "\n",
    "def weather_forecast_tool(country: str, city: str, date: str) -> str:\n",
    "    if country.lower() in {\"united kingdom\", \"uk\", \"england\"}:\n",
    "        return \"Fog and rain\"\n",
    "    else:\n",
    "        return \"Sunshine\"\n",
    "\n",
    "\n",
    "prompt = \"What will be weather in Birmingham in two weeks?\"\n",
    "response = make_llm_request(prompt, *get_tool_definitions())\n",
    "print(\"Response:\\n\", response)\n",
    "\n",
    "print()\n",
    "\n",
    "prompt = \"What will be weather in Warsaw the day after tomorrow?\"\n",
    "response = make_llm_request(prompt, *get_tool_definitions())\n",
    "print(\"Response:\\n\", response)\n",
    "\n",
    "print()\n",
    "\n",
    "prompt = \"What will be weather in New York in two months?\"\n",
    "response = make_llm_request(prompt, *get_tool_definitions())\n",
    "print(\"Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11f9b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def get_dataset_tool_definitions() -> tuple[list[dict], dict]:\n",
    "    tool_definitions = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"read_remote_csv\",\n",
    "                \"description\": \"Read a CSV file from a URL and return the first n rows as text. n can be at most 20\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"url\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Public URL to a CSV file\",\n",
    "                        },\n",
    "                        \"n\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"Maximum number of rows to return\",\n",
    "                            \"default\": 50,\n",
    "                            \"minimum\": 0,\n",
    "                            \"maximum\": 20\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"url\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"read_remote_parquet\",\n",
    "                \"description\": \"Read a Parquet file from a URL and return the first n rows as text. n can be at most 20\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"url\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Public URL to a Parquet file\",\n",
    "                        },\n",
    "                        \"n\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"Maximum number of rows to return\",\n",
    "                            \"default\": 50,\n",
    "                            \"minimum\": 0,\n",
    "                            \"maximum\": 20\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"url\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    tool_name_to_callable = {\n",
    "        \"read_remote_csv\": read_remote_csv,\n",
    "        \"read_remote_parquet\": read_remote_parquet,\n",
    "    }\n",
    "\n",
    "    return tool_definitions, tool_name_to_callable\n",
    "\n",
    "def read_remote_csv(url: str, n: int) -> str:\n",
    "    n = min(n, 20)\n",
    "    try:\n",
    "        df = pl.read_csv(\n",
    "            url,\n",
    "            n_rows=n,\n",
    "            ignore_errors=True,\n",
    "        )\n",
    "        return str(df.to_dicts())\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR: Failed to read CSV from URL: {url}.\\n\"\n",
    "            f\"Reason: {type(e).__name__}: {e}\\n\"\n",
    "        )\n",
    "    \n",
    "\n",
    "def read_remote_parquet(url: str, n: int) -> str:\n",
    "    n = min(n, 20)\n",
    "    try:\n",
    "        df = pl.read_parquet(\n",
    "            url,\n",
    "            n_rows=n,\n",
    "        )\n",
    "        return str(df.to_dicts())\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR: Failed to read CSV from URL: {url}.\\n\"\n",
    "            f\"Reason: {type(e).__name__}: {e}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "572b54db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated message: {'content': None, 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'chatcmpl-tool-a827e3edb39513b1', 'function': {'arguments': '{\"url\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet\", \"n\": 20}', 'name': 'read_remote_parquet'}, 'type': 'function'}], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Generated message: {'content': 'The average ride cost in NYC for January 2025 is calculated by summing up all the `total_amount` values and dividing by the number of rides. \\n\\nFrom the data provided, the `total_amount` values are as follows:\\n\\n1. 18.0\\n2. 12.12\\n3. 12.1\\n4. 9.7\\n5. 8.3\\n6. 24.1\\n7. 11.75\\n8. 19.1\\n9. 27.1\\n10. 16.4\\n11. 16.4\\n12. 12.96\\n13. 19.2\\n14. 12.9\\n15. 38.9\\n16. 22.7\\n17. 25.55\\n18. -8.54\\n19. 12.2\\n20. 20.6\\n\\nSumming these values:\\n\\n$$\\n18.0 + 12.12 + 12.1 + 9.7 + 8.3 + 24.1 + 11.75 + 19.1 + 27.1 + 16.4 + 16.4 + 12.96 + 19.2 + 12.9 + 38.9 + 22.7 + 25.55 - 8.54 + 12.2 + 20.6\\n$$\\n\\n$$\\n= 239.0\\n$$\\n\\nNumber of rides: 20\\n\\nAverage ride cost:\\n\\n$$\\n\\\\frac{239.0}{20} = 11.95\\n$$\\n\\nSo, the average ride cost in NYC for January 2025 is **$11.95**.', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None}\n",
      "\n",
      "Response:\n",
      " The average ride cost in NYC for January 2025 is calculated by summing up all the `total_amount` values and dividing by the number of rides. \n",
      "\n",
      "From the data provided, the `total_amount` values are as follows:\n",
      "\n",
      "1. 18.0\n",
      "2. 12.12\n",
      "3. 12.1\n",
      "4. 9.7\n",
      "5. 8.3\n",
      "6. 24.1\n",
      "7. 11.75\n",
      "8. 19.1\n",
      "9. 27.1\n",
      "10. 16.4\n",
      "11. 16.4\n",
      "12. 12.96\n",
      "13. 19.2\n",
      "14. 12.9\n",
      "15. 38.9\n",
      "16. 22.7\n",
      "17. 25.55\n",
      "18. -8.54\n",
      "19. 12.2\n",
      "20. 20.6\n",
      "\n",
      "Summing these values:\n",
      "\n",
      "$$\n",
      "18.0 + 12.12 + 12.1 + 9.7 + 8.3 + 24.1 + 11.75 + 19.1 + 27.1 + 16.4 + 16.4 + 12.96 + 19.2 + 12.9 + 38.9 + 22.7 + 25.55 - 8.54 + 12.2 + 20.6\n",
      "$$\n",
      "\n",
      "$$\n",
      "= 239.0\n",
      "$$\n",
      "\n",
      "Number of rides: 20\n",
      "\n",
      "Average ride cost:\n",
      "\n",
      "$$\n",
      "\\frac{239.0}{20} = 11.95\n",
      "$$\n",
      "\n",
      "So, the average ride cost in NYC for January 2025 is **$11.95**.\n"
     ]
    }
   ],
   "source": [
    "apis_tox_url = \"https://raw.githubusercontent.com/j-adamczyk/ApisTox_dataset/master/outputs/dataset_final.csv\"\n",
    "taxi_data_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet\"\n",
    "\n",
    "\n",
    "prompt = (\n",
    "    f\"Here you have a daset on taxi rides: {taxi_data_url} \"\n",
    "    f\"How much did the average ride cost in NYC taxi cost in January 2025\"\n",
    ")\n",
    "\n",
    "response = make_llm_request(prompt, *get_dataset_tool_definitions())\n",
    "print(\"Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d2876",
   "metadata": {},
   "source": [
    "## Excercise 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-course-agh-lab-13 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
